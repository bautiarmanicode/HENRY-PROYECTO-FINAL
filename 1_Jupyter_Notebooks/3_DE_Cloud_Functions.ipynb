{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🛠️Automatización del ETL \n",
    "\n",
    "- Se realizó la automatización de la limpieza y transformación  de los archivos que subimos al bucket denominado ` pizza-henry ` en la dirección ` \\RAW\\Yelp ` y posteriormente la carga haciael bucket ` pizza-henry-staged ` en la dirección ` \\STAGED\\ `.\n",
    "- Este proceso nos ahorra tiempo y reduce la posibilidad de errores humano, por esa misma razòn, el còdigo es  ` eficienciente `\n",
    "- Este código es ` escalable `, ya que se puede adaptar para manejar gran volumen de datos y diferentes tipos de análisis con mínimas modificaciones "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yelp📍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✅Etl de yelp_business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions_framework\n",
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "import os\n",
    "from io import BytesIO\n",
    "import re\n",
    "\n",
    "# Inicializa el cliente de Google Cloud Storage\n",
    "storage_client = storage.Client()\n",
    "\n",
    "def process_and_upload_data(bucket_name, source_blob_name, destination_bucket_name):\n",
    "    try:\n",
    "        print(f\"Procesando archivo desde {bucket_name}/{source_blob_name}\")\n",
    "\n",
    "        # Descargar el archivo\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(source_blob_name)\n",
    "        pkl_data = blob.download_as_bytes()\n",
    "        \n",
    "        # Cargar los datos en un DataFrame\n",
    "        temp_file = BytesIO(pkl_data)\n",
    "        df_business = pd.read_pickle(temp_file)\n",
    "        \n",
    "        # Procesamiento específico\n",
    "        df_business = df_business.loc[:, ~df_business.columns.duplicated()]\n",
    "        df_business = df_business.dropna(subset=['categories'])\n",
    "        df_restaurantes = df_business[df_business['categories'].str.lower().str.contains('restaurant')].reset_index(drop=True)\n",
    "        \n",
    "        # Filtros geográficos y categorías\n",
    "        latitud_max, latitud_min = 31.0, 24.5\n",
    "        longitud_max, longitud_min = -80.0, -87.6\n",
    "        mascara_latitud = (df_restaurantes['latitude'] >= latitud_min) & (df_restaurantes['latitude'] <= latitud_max)\n",
    "        mascara_longitud = (df_restaurantes['longitude'] >= longitud_min) & (df_restaurantes['longitude'] <= longitud_max)\n",
    "        df_restaurantes_FL = df_restaurantes[mascara_latitud & mascara_longitud]\n",
    "        df_restaurantes_FL = df_restaurantes_FL[df_restaurantes_FL['is_open'] == 1].reset_index(drop=True)\n",
    "        df_restaurantes_FL.drop(columns=['is_open', 'state'], inplace=True)\n",
    "        \n",
    "        # Ajustes adicionales y carga\n",
    "        df_restaurantes_FL = df_restaurantes_FL.drop(columns=['postal_code', 'attributes', 'hours'])\n",
    "        \n",
    "        # Filtrar pizzerías y Taco Bell en Florida\n",
    "        pizza_pattern = re.compile(r'pizza|pizzería', re.IGNORECASE)\n",
    "        taco_bell_pattern = re.compile(r'taco\\s*bell', re.IGNORECASE)\n",
    "\n",
    "        df_filtered = df_restaurantes_FL[\n",
    "            df_restaurantes_FL['categories'].str.contains(pizza_pattern) | \n",
    "            df_restaurantes_FL['name'].apply(lambda x: bool(pizza_pattern.search(x))) |\n",
    "            df_restaurantes_FL['name'].apply(lambda x: bool(taco_bell_pattern.search(x)))\n",
    "        ]\n",
    "\n",
    "        # Verificar que todas las filas tengan el mismo número de columnas\n",
    "        expected_columns = 9\n",
    "        df_filtered = df_filtered.loc[:, df_filtered.columns[:expected_columns]]\n",
    "\n",
    "        # Guardar y cargar CSV\n",
    "        buffer = BytesIO()\n",
    "        df_filtered.to_csv(buffer, index=False)\n",
    "        buffer.seek(0)\n",
    "        destination_blob_name = 'STAGED/Yelp/business_filtered_FL.csv'\n",
    "        destination_blob = storage_client.bucket(destination_bucket_name).blob(destination_blob_name)\n",
    "        destination_blob.upload_from_file(buffer, content_type='text/csv')\n",
    "        print(f\"Datos de pizzerías, Pizza Hut y Taco Bell en Florida cargados exitosamente a {destination_blob_name} en el bucket {destination_bucket_name}.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "# Uso de variables de entorno para definir los parámetros\n",
    "source_bucket_name = os.getenv('SOURCE_BUCKET_NAME', 'henry-pizza')\n",
    "source_blob_name = os.getenv('SOURCE_BLOB_NAME', 'RAW/Yelp/business.pkl')\n",
    "destination_bucket_name = os.getenv('DESTINATION_BUCKET_NAME', 'henry-pizza-staged')\n",
    "\n",
    "# Triggered by a change in a storage bucket\n",
    "@functions_framework.cloud_event\n",
    "def business_etl(cloud_event):\n",
    "    data = cloud_event.data\n",
    "\n",
    "    event_id = cloud_event[\"id\"]\n",
    "    event_type = cloud_event[\"type\"]\n",
    "\n",
    "    bucket = data[\"bucket\"]\n",
    "    name = data[\"name\"]\n",
    "    metageneration = data[\"metageneration\"]\n",
    "    timeCreated = data[\"timeCreated\"]\n",
    "    updated = data[\"updated\"]\n",
    "\n",
    "    print(f\"Event ID: {event_id}\")\n",
    "    print(f\"Event type: {event_type}\")\n",
    "    print(f\"Bucket: {bucket}\")\n",
    "    print(f\"File: {name}\")\n",
    "    print(f\"Metageneration: {metageneration}\")\n",
    "    print(f\"Created: {timeCreated}\")\n",
    "    print(f\"Updated: {updated}\")\n",
    "\n",
    "    # Verificar si el archivo que disparó el evento es el archivo correcto\n",
    "    if name == 'RAW/Yelp/business.pkl':\n",
    "        print(\"Archivo correcto detectado para procesamiento.\")\n",
    "        process_and_upload_data(source_bucket_name, source_blob_name, destination_bucket_name)\n",
    "    else:\n",
    "        print(f\"Archivo {name} no es business.pkl. Procesamiento no necesario.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✅Etl de Yelp_checkin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions_framework\n",
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "import os\n",
    "from io import BytesIO\n",
    "\n",
    "# Inicializa el cliente de Google Cloud Storage\n",
    "storage_client = storage.Client()\n",
    "\n",
    "def validate_data(df, table_name):\n",
    "    \"\"\"\n",
    "    Función para validar los datos antes de continuar con el procesamiento.\n",
    "    \"\"\"\n",
    "    if df.isnull().values.any():\n",
    "        print(f\"Warning: {table_name} contiene valores nulos.\")\n",
    "        df = df.dropna()\n",
    "    if df.duplicated().any():\n",
    "        print(f\"Warning: {table_name} contiene duplicados.\")\n",
    "        df = df.drop_duplicates()\n",
    "    return df\n",
    "\n",
    "def get_business_ids_from_filtered_file(bucket_name, blob_name):\n",
    "    \"\"\"\n",
    "    Función para obtener los IDs de negocios filtrados desde un archivo CSV en Google Cloud Storage.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(blob_name)\n",
    "        if not blob.exists():\n",
    "            raise FileNotFoundError(f\"El archivo {blob_name} no existe en el bucket {bucket_name}\")\n",
    "        \n",
    "        csv_data = blob.download_as_bytes()\n",
    "        temp_file = BytesIO(csv_data)\n",
    "        df_filtered_business = pd.read_csv(temp_file)\n",
    "        return df_filtered_business['business_id'].unique()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while fetching business IDs: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def process_and_upload_data(bucket_name, source_blob_name, destination_bucket_name, business_ids):\n",
    "    try:\n",
    "        print(f\"Procesando archivo desde {bucket_name}/{source_blob_name}\")\n",
    "\n",
    "        # Descargar el archivo\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(source_blob_name)\n",
    "        json_data = blob.download_as_bytes()\n",
    "        print(f\"Archivo {source_blob_name} descargado del bucket {bucket_name}\")\n",
    "\n",
    "        # Verificar si los datos están vacíos\n",
    "        if not json_data:\n",
    "            raise ValueError(\"El archivo JSON está vacío.\")\n",
    "\n",
    "        # Intentar cargar los datos en un DataFrame usando diferentes codificaciones\n",
    "        temp_file = BytesIO(json_data)\n",
    "        try:\n",
    "            df_checkin = pd.read_json(temp_file, lines=True)\n",
    "        except ValueError as e:\n",
    "            print(f\"Error al leer el archivo JSON: {e}\")\n",
    "            raise \n",
    "        except UnicodeDecodeError as e: \n",
    "            print(f\"Error de decodificación: {e}\")\n",
    "            raise\n",
    "\n",
    "        print(f\"Archivo {source_blob_name} cargado en DataFrame\")\n",
    "\n",
    "        # Asegurarse de que la columna \"date\" es de tipo string\n",
    "        df_checkin[\"date\"] = df_checkin[\"date\"].astype(str)\n",
    "\n",
    "        # Procesamiento específico\n",
    "        df_checkin[\"date\"] = df_checkin[\"date\"].str.split(',')\n",
    "        df_checkin = df_checkin.explode(\"date\").reset_index(drop=True)\n",
    "        df_checkin[\"date\"] = df_checkin[\"date\"].str.strip()\n",
    "\n",
    "        # Intentar convertir a datetime con un formato explícito\n",
    "        date_formats = [\"%Y-%m-%d %H:%M:%S\", \"%Y-%m-%d\"]  # Lista de formatos esperados\n",
    "        for fmt in date_formats:\n",
    "            try:\n",
    "                df_checkin[\"date\"] = pd.to_datetime(df_checkin[\"date\"], format=fmt, errors='raise')\n",
    "                break\n",
    "            except ValueError:\n",
    "                continue\n",
    "        else:\n",
    "            df_checkin[\"date\"] = pd.to_datetime(df_checkin[\"date\"], errors='coerce')\n",
    "\n",
    "        # Filtrar por business_id de pizzerías en Florida y Taco Bell\n",
    "        df_checkin_filtered = df_checkin[df_checkin[\"business_id\"].isin(business_ids)]\n",
    "        print(f\"Datos filtrados. Dimensiones del dataframe: {df_checkin_filtered.shape}\")\n",
    "\n",
    "        # Validar y convertir tipos de columnas\n",
    "        df_checkin_filtered = validate_data(df_checkin_filtered, 'checkin')\n",
    "\n",
    "        # Guardar y cargar CSV\n",
    "        buffer = BytesIO()\n",
    "        df_checkin_filtered.to_csv(buffer, index=False)\n",
    "        buffer.seek(0)\n",
    "        destination_blob_name = source_blob_name.replace('RAW', 'STAGED').replace('.json', '_etl.csv')\n",
    "        destination_blob = storage_client.bucket(destination_bucket_name).blob(destination_blob_name)\n",
    "        destination_blob.upload_from_file(buffer, content_type='text/csv')\n",
    "        print(f\"Datos filtrados cargados exitosamente a {destination_blob_name} en el bucket {destination_bucket_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "# Uso de variables de entorno para definir los parámetros\n",
    "source_bucket_name = os.getenv('SOURCE_BUCKET_NAME', 'henry-pizza')\n",
    "source_blob_name = os.getenv('SOURCE_BLOB_NAME', 'RAW/Yelp/checkin.json')\n",
    "destination_bucket_name = os.getenv('DESTINATION_BUCKET_NAME', 'henry-pizza-staged')\n",
    "filtered_business_blob_name = 'STAGED/Yelp/business_filtered_FL.csv'\n",
    "\n",
    "@functions_framework.cloud_event\n",
    "def checkin_etl(cloud_event):\n",
    "    try:\n",
    "        data = cloud_event.data\n",
    "\n",
    "        event_id = cloud_event.get(\"id\")\n",
    "        event_type = cloud_event.get(\"type\")\n",
    "\n",
    "        bucket = data[\"bucket\"]\n",
    "        name = data[\"name\"]\n",
    "        metageneration = data[\"metageneration\"]\n",
    "        timeCreated = data[\"timeCreated\"]\n",
    "        updated = data[\"updated\"]\n",
    "\n",
    "        print(f\"Event ID: {event_id}\")\n",
    "        print(f\"Event type: {event_type}\")\n",
    "        print(f\"Bucket: {bucket}\")\n",
    "        print(f\"File: {name}\")\n",
    "        print(f\"Metageneration: {metageneration}\")\n",
    "        print(f\"Created: {timeCreated}\")\n",
    "        print(f\"Updated: {updated}\")\n",
    "\n",
    "        # Obtener business_ids de pizzerías en Florida y Taco Bell\n",
    "        business_ids = get_business_ids_from_filtered_file(destination_bucket_name, filtered_business_blob_name)\n",
    "        print(f\"Total de Business IDs obtenidos: {len(business_ids)}\")\n",
    "\n",
    "        # Procesar y subir los datos si el archivo es checkin.json\n",
    "        if name == 'RAW/Yelp/checkin.json':\n",
    "            print(\"Archivo correcto detectado para procesamiento.\")\n",
    "            process_and_upload_data(source_bucket_name, name, destination_bucket_name, business_ids)\n",
    "        else:\n",
    "            print(f\"Archivo {name} no es relevante para este procesamiento. Procesamiento no necesario.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✅Etl de tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions_framework\n",
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "import os\n",
    "from io import BytesIO\n",
    "import csv\n",
    "\n",
    "# Inicializa el cliente de Google Cloud Storage\n",
    "storage_client = storage.Client()\n",
    "\n",
    "def get_business_ids_from_filtered_file(bucket_name, blob_name):\n",
    "    try:\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(blob_name)\n",
    "        if not blob.exists():\n",
    "            raise FileNotFoundError(f\"El archivo {blob_name} no existe en el bucket {bucket_name}\")\n",
    "        \n",
    "        csv_data = blob.download_as_bytes()\n",
    "        temp_file = BytesIO(csv_data)\n",
    "        df_filtered_business = pd.read_csv(temp_file)\n",
    "        return df_filtered_business['business_id'].unique()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while fetching business IDs: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean text to ensure proper CSV formatting\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        text = text.replace('\"', '\"\"')  # Escapar comillas dobles\n",
    "        text = text.replace('\\n', ' ')  # Eliminar saltos de línea\n",
    "        text = text.replace('\\r', ' ')  # Eliminar retornos de carro\n",
    "    return text\n",
    "\n",
    "def process_and_upload_data(bucket_name, source_blob_name, destination_bucket_name, business_ids):\n",
    "    try:\n",
    "        print(f\"Procesando archivo desde {bucket_name}/{source_blob_name} y subiendo a {destination_bucket_name}\")\n",
    "\n",
    "        # Descargar el archivo\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(source_blob_name)\n",
    "        json_data = blob.download_as_bytes()\n",
    "        print(f\"Archivo {source_blob_name} descargado del bucket {bucket_name}\")\n",
    "\n",
    "        # Verificar si los datos están vacíos\n",
    "        if not json_data:\n",
    "            raise ValueError(\"El archivo JSON está vacío.\")\n",
    "\n",
    "        # Cargar los datos en un DataFrame\n",
    "        temp_file = BytesIO(json_data)\n",
    "        try:\n",
    "            df_tip = pd.read_json(temp_file, lines=True)\n",
    "            print(f\"Archivo JSON cargado en el DataFrame.\")\n",
    "        except ValueError as ve:\n",
    "            raise ValueError(f\"Error al leer el archivo JSON: {ve}\")\n",
    "\n",
    "        # Filtrar por business_id de pizzerías en Florida y Taco Bell\n",
    "        df_tip_filtered = df_tip[df_tip[\"business_id\"].isin(business_ids)]\n",
    "        print(f\"Datos filtrados. Dimensiones del dataframe: {df_tip_filtered.shape}\")\n",
    "\n",
    "        # Procesamiento específico\n",
    "        df_tip_filtered = df_tip_filtered.dropna()\n",
    "        df_tip_filtered = df_tip_filtered.drop_duplicates()\n",
    "\n",
    "        # Limpiar texto para asegurar formato adecuado de CSV\n",
    "        df_tip_filtered['text'] = df_tip_filtered['text'].apply(clean_text)\n",
    "\n",
    "        # Convertir la columna de fecha a string en formato de fecha\n",
    "        df_tip_filtered['date'] = pd.to_datetime(df_tip_filtered['date']).dt.date\n",
    "\n",
    "        # Agregar columna tip_id como un índice\n",
    "        #df_tip_filtered['tip_id'] = df_tip_filtered.index.astype(str)\n",
    "\n",
    "        # Asegurarse de que todas las columnas tengan el tipo de datos correcto\n",
    "        df_tip_filtered['user_id'] = df_tip_filtered['user_id'].astype(str)\n",
    "        df_tip_filtered['business_id'] = df_tip_filtered['business_id'].astype(str)\n",
    "        df_tip_filtered['text'] = df_tip_filtered['text'].astype(str)\n",
    "        #df_tip_filtered['tip_id'] = df_tip_filtered['tip_id'].astype(str)\n",
    "        df_tip_filtered['date'] = df_tip_filtered['date'].astype(str)  # Convertir fecha a string\n",
    "\n",
    "        # Guardar y cargar CSV\n",
    "        buffer = BytesIO()\n",
    "        df_tip_filtered.to_csv(buffer, index=False, quoting=csv.QUOTE_NONNUMERIC)\n",
    "        buffer.seek(0)\n",
    "        destination_blob_name = source_blob_name.replace('RAW', 'STAGED').replace('.json', '_etl.csv')\n",
    "        destination_blob = storage_client.bucket(destination_bucket_name).blob(destination_blob_name)\n",
    "        destination_blob.upload_from_file(buffer, content_type='text/csv')\n",
    "        print(f\"Datos cargados exitosamente a {destination_blob_name} en el bucket {destination_bucket_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "# Uso de variables de entorno para definir los parámetros\n",
    "source_bucket_name = os.getenv('SOURCE_BUCKET_NAME', 'henry-pizza')\n",
    "source_blob_name = os.getenv('SOURCE_BLOB_NAME', 'RAW/Yelp/tip.json')\n",
    "destination_bucket_name = os.getenv('DESTINATION_BUCKET_NAME', 'henry-pizza-staged')\n",
    "filtered_business_blob_name = 'STAGED/Yelp/business_filtered_FL.csv'\n",
    "\n",
    "@functions_framework.cloud_event\n",
    "def etl_tip(cloud_event):\n",
    "    try:\n",
    "        data = cloud_event.data\n",
    "\n",
    "        event_id = cloud_event.get(\"id\")\n",
    "        event_type = cloud_event.get(\"type\")\n",
    "\n",
    "        bucket = data[\"bucket\"]\n",
    "        name = data[\"name\"]\n",
    "        metageneration = data[\"metageneration\"]\n",
    "        timeCreated = data.get(\"timeCreated\")\n",
    "        updated = data.get(\"updated\")\n",
    "\n",
    "        print(f\"Event ID: {event_id}\")\n",
    "        print(f\"Event type: {event_type}\")\n",
    "        print(f\"Bucket: {bucket}\")\n",
    "        print(f\"File: {name}\")\n",
    "        print(f\"Metageneration: {metageneration}\")\n",
    "        print(f\"Created: {timeCreated}\")\n",
    "        print(f\"Updated: {updated}\")\n",
    "\n",
    "        # Obtener business_ids de pizzerías en Florida y Taco Bell\n",
    "        business_ids = get_business_ids_from_filtered_file(destination_bucket_name, filtered_business_blob_name)\n",
    "        print(f\"Total de Business IDs obtenidos: {len(business_ids)}\")\n",
    "\n",
    "        # Procesar y subir los datos si el archivo es tip.json\n",
    "        if name == 'RAW/Yelp/tip.json':\n",
    "            print(\"Archivo correcto detectado para procesamiento.\")\n",
    "            process_and_upload_data(source_bucket_name, name, destination_bucket_name, business_ids)\n",
    "        else:\n",
    "            print(f\"Archivo {name} no es relevante para este procesamiento. Procesamiento no necesario.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✅Etl de Yelp_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions_framework\n",
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "import os\n",
    "from io import BytesIO, StringIO\n",
    "import json\n",
    "\n",
    "# Inicializa el cliente de Google Cloud Storage\n",
    "storage_client = storage.Client()\n",
    "\n",
    "def get_business_ids_from_filtered_file(bucket_name, blob_name):\n",
    "    \"\"\"\n",
    "    Obtiene los IDs de negocios filtrados desde un archivo CSV en Google Cloud Storage.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Accede al bucket y al blob (archivo) especificado\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(blob_name)\n",
    "        \n",
    "        # Verifica si el archivo existe\n",
    "        if not blob.exists():\n",
    "            raise FileNotFoundError(f\"El archivo {blob_name} no existe en el bucket {bucket_name}\")\n",
    "        \n",
    "        # Descarga y lee el archivo CSV en un DataFrame\n",
    "        csv_data = blob.download_as_bytes()\n",
    "        temp_file = BytesIO(csv_data)\n",
    "        df_filtered_business = pd.read_csv(temp_file)\n",
    "        \n",
    "        # Retorna los IDs únicos de los negocios\n",
    "        return df_filtered_business['business_id'].unique()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while fetching business IDs: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def save_filtered_data(filtered_reviews, source_blob_name, destination_bucket_name, chunk_index):\n",
    "    \"\"\"\n",
    "    Guarda los datos filtrados en un archivo CSV.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Crea un DataFrame con los datos filtrados\n",
    "        df_filtered = pd.DataFrame(filtered_reviews)\n",
    "        \n",
    "        # Valida y convierte los tipos de datos\n",
    "        df_filtered['review_id'] = df_filtered['review_id'].astype(str)\n",
    "        df_filtered['user_id'] = df_filtered['user_id'].astype(str)\n",
    "        df_filtered['business_id'] = df_filtered['business_id'].astype(str)\n",
    "        df_filtered['stars'] = df_filtered['stars'].astype(float)\n",
    "        df_filtered['date'] = pd.to_datetime(df_filtered['date']).dt.date\n",
    "        df_filtered['text'] = df_filtered['text'].astype(str)\n",
    "        df_filtered['useful'] = df_filtered['useful'].astype(int)\n",
    "        df_filtered['funny'] = df_filtered['funny'].astype(int)\n",
    "        df_filtered['cool'] = df_filtered['cool'].astype(int)\n",
    "\n",
    "        # Guarda los datos filtrados en un archivo CSV\n",
    "        destination_blob_name = source_blob_name.replace('RAW', 'STAGED').replace('.json', f'_etl_part_{chunk_index}.csv')\n",
    "        buffer = StringIO()\n",
    "        df_filtered.to_csv(buffer, index=False)\n",
    "        buffer.seek(0)\n",
    "        destination_bucket = storage_client.bucket(destination_bucket_name)\n",
    "        destination_blob = destination_bucket.blob(destination_blob_name)\n",
    "        destination_blob.upload_from_string(buffer.getvalue(), content_type='text/csv')\n",
    "        print(f\"Datos filtrados cargados exitosamente a {destination_blob_name} en el bucket {destination_bucket_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while saving filtered data: {str(e)}\")\n",
    "\n",
    "def process_and_upload_data(bucket_name, source_blob_name, destination_bucket_name, business_ids, chunk_size=5000):\n",
    "    \"\"\"\n",
    "    Procesa y carga datos en chunks para reducir el uso de memoria.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Procesando archivo desde {bucket_name}/{source_blob_name}\")\n",
    "\n",
    "        # Accede al bucket y al blob (archivo) especificado\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(source_blob_name)\n",
    "\n",
    "        # Descarga el contenido del blob línea por línea\n",
    "        with blob.open(\"r\") as blob_reader:\n",
    "            filtered_reviews = []\n",
    "            chunk_index = 1\n",
    "\n",
    "            for line in blob_reader:\n",
    "                review = json.loads(line)\n",
    "                if review['business_id'] in business_ids:\n",
    "                    filtered_reviews.append(review)\n",
    "                if len(filtered_reviews) >= chunk_size:\n",
    "                    save_filtered_data(filtered_reviews, source_blob_name, destination_bucket_name, chunk_index)\n",
    "                    filtered_reviews = []  # Limpia la lista para liberar memoria\n",
    "                    chunk_index += 1\n",
    "\n",
    "            # Guarda cualquier resto de datos filtrados\n",
    "            if filtered_reviews:\n",
    "                save_filtered_data(filtered_reviews, source_blob_name, destination_bucket_name, chunk_index)\n",
    "                #Se cargó exitosamente\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing data: {str(e)}\")\n",
    "\n",
    "# Uso de variables de entorno para definir los parámetros\n",
    "source_bucket_name = os.getenv('SOURCE_BUCKET_NAME', 'henry-pizza')\n",
    "source_blob_name = os.getenv('SOURCE_BLOB_NAME', 'RAW/Yelp/review.json')\n",
    "destination_bucket_name = os.getenv('DESTINATION_BUCKET_NAME', 'henry-pizza-staged')\n",
    "filtered_business_blob_name = 'STAGED/Yelp/business_filtered_FL.csv'\n",
    "\n",
    "@functions_framework.cloud_event\n",
    "def etl_review(cloud_event):\n",
    "    \"\"\"\n",
    "    Cloud Function para procesar y cargar datos de reviews.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = cloud_event.data\n",
    "\n",
    "        event_id = cloud_event.get(\"id\")\n",
    "        event_type = cloud_event.get(\"type\")\n",
    "\n",
    "        bucket = data[\"bucket\"]\n",
    "        name = data[\"name\"]\n",
    "        metageneration = data[\"metageneration\"]\n",
    "        timeCreated = data[\"timeCreated\"]\n",
    "        updated = data[\"updated\"]\n",
    "\n",
    "        print(f\"Event ID: {event_id}\")\n",
    "        print(f\"Event type: {event_type}\")\n",
    "        print(f\"Bucket: {bucket}\")\n",
    "        print(f\"File: {name}\")\n",
    "        print(f\"Metageneration: {metageneration}\")\n",
    "        print(f\"Created: {timeCreated}\")\n",
    "        print(f\"Updated: {updated}\")\n",
    "\n",
    "        # Obtener business_ids de pizzerías en Florida y Taco Bell\n",
    "        business_ids = get_business_ids_from_filtered_file(destination_bucket_name, filtered_business_blob_name)\n",
    "        print(f\"Total de Business IDs obtenidos: {len(business_ids)}\")\n",
    "\n",
    "        # Verificar si el archivo que disparó el evento es el archivo correcto\n",
    "        if name == 'RAW/Yelp/review.json':\n",
    "            print(\"Archivo correcto detectado para procesamiento.\")\n",
    "            # Procesa y sube los datos en un solo archivo CSV\n",
    "            process_and_upload_data(source_bucket_name, source_blob_name, destination_bucket_name, business_ids)\n",
    "        else:\n",
    "            print(f\"Archivo {name} no es relevante para este procesamiento. Procesamiento no necesario.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✅Etl de Yelp_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions_framework\n",
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "import os\n",
    "from io import BytesIO\n",
    "\n",
    "# Inicializa el cliente de Google Cloud Storage\n",
    "storage_client = storage.Client()\n",
    "\n",
    "def validate_data(df, table_name):\n",
    "    \"\"\"\n",
    "    Función para validar los datos antes de continuar con el procesamiento.\n",
    "    \"\"\"\n",
    "    if df.isnull().values.any():\n",
    "        print(f\"Warning: {table_name} contiene valores nulos.\")\n",
    "        df = df.dropna()\n",
    "    if df.duplicated().any():\n",
    "        print(f\"Warning: {table_name} contiene duplicados.\")\n",
    "        df = df.drop_duplicates()\n",
    "    return df\n",
    "\n",
    "def convert_column_types(df):\n",
    "    \"\"\"\n",
    "    Convierte los tipos de columna a los tipos adecuados para su posterior uso.\n",
    "    \"\"\"\n",
    "    df['user_id'] = df['user_id'].astype(str)\n",
    "    df['name'] = df['name'].astype(str)\n",
    "    df['review_count'] = df['review_count'].astype(int)\n",
    "    df['yelping_since'] = pd.to_datetime(df['yelping_since'], errors='coerce')\n",
    "    df['useful'] = df['useful'].astype(int)\n",
    "    df['funny'] = df['funny'].astype(int)\n",
    "    df['cool'] = df['cool'].astype(int)\n",
    "    df['fans'] = df['fans'].astype(int)\n",
    "    df['elite'] = df['elite'].astype(str)\n",
    "    df['average_stars'] = df['average_stars'].astype(float)\n",
    "    df['compliment_hot'] = df['compliment_hot'].astype(int)\n",
    "    df['compliment_more'] = df['compliment_more'].astype(int)\n",
    "    df['compliment_profile'] = df['compliment_profile'].astype(int)\n",
    "    df['compliment_cute'] = df['compliment_cute'].astype(int)\n",
    "    df['compliment_list'] = df['compliment_list'].astype(int)\n",
    "    df['compliment_note'] = df['compliment_note'].astype(int)\n",
    "    df['compliment_plain'] = df['compliment_plain'].astype(int)\n",
    "    df['compliment_cool'] = df['compliment_cool'].astype(int)\n",
    "    df['compliment_funny'] = df['compliment_funny'].astype(int)\n",
    "    df['compliment_writer'] = df['compliment_writer'].astype(int)\n",
    "    df['compliment_photos'] = df['compliment_photos'].astype(int)\n",
    "    return df\n",
    "\n",
    "def process_and_upload_data(bucket_name, source_blob_name, destination_bucket_name, chunk_size=50000):\n",
    "    try:\n",
    "        print(f\"Procesando archivo desde {bucket_name}/{source_blob_name}\")\n",
    "\n",
    "        # Descargar el archivo\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(source_blob_name)\n",
    "        parquet_data = blob.download_as_bytes()\n",
    "        print(f\"Archivo {source_blob_name} descargado del bucket {bucket_name}\")\n",
    "\n",
    "        # Verificar si los datos están vacíos\n",
    "        if not parquet_data:\n",
    "            raise ValueError(\"El archivo Parquet está vacío.\")\n",
    "\n",
    "        # Cargar los datos en un DataFrame desde un archivo Parquet\n",
    "        temp_file = BytesIO(parquet_data)\n",
    "        df_user = pd.read_parquet(temp_file)\n",
    "        print(f\"Archivo {source_blob_name} cargado en DataFrame\")\n",
    "        print(f\"Primeras filas del DataFrame original:\\n{df_user.head()}\")\n",
    "\n",
    "        # Validar y convertir tipos de columnas\n",
    "        df_user = validate_data(df_user, 'user')\n",
    "        df_user = convert_column_types(df_user)\n",
    "\n",
    "        # Guardar y cargar CSV\n",
    "        buffer = BytesIO()\n",
    "        df_user.to_csv(buffer, index=False)\n",
    "        buffer.seek(0)\n",
    "        destination_blob_name = source_blob_name.replace('RAW', 'STAGED').replace('.parquet', '_etl.csv')\n",
    "        destination_blob = storage_client.bucket(destination_bucket_name).blob(destination_blob_name)\n",
    "        destination_blob.upload_from_file(buffer, content_type='text/csv')\n",
    "        print(f\"Datos cargados exitosamente a {destination_blob_name} en el bucket {destination_bucket_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "# Uso de variables de entorno para definir los parámetros\n",
    "source_bucket_name = os.getenv('SOURCE_BUCKET_NAME', 'henry-pizza')\n",
    "source_blob_name = os.getenv('SOURCE_BLOB_NAME', 'RAW/Yelp/user.parquet')\n",
    "destination_bucket_name = os.getenv('DESTINATION_BUCKET_NAME', 'henry-pizza-staged')\n",
    "\n",
    "@functions_framework.cloud_event\n",
    "def etl_user(cloud_event):\n",
    "    try:\n",
    "        data = cloud_event.data\n",
    "\n",
    "        event_id = cloud_event.get(\"id\")\n",
    "        event_type = cloud_event.get(\"type\")\n",
    "\n",
    "        bucket = data[\"bucket\"]\n",
    "        name = data[\"name\"]\n",
    "        metageneration = data[\"metageneration\"]\n",
    "        timeCreated = data[\"timeCreated\"]\n",
    "        updated = data[\"updated\"]\n",
    "\n",
    "        print(f\"Event ID: {event_id}\")\n",
    "        print(f\"Event type: {event_type}\")\n",
    "        print(f\"Bucket: {bucket}\")\n",
    "        print(f\"File: {name}\")\n",
    "        print(f\"Metageneration: {metageneration}\")\n",
    "        print(f\"Created: {timeCreated}\")\n",
    "        print(f\"Updated: {updated}\")\n",
    "\n",
    "        # Procesar y subir los datos si el archivo es user.parquet\n",
    "        if name == source_blob_name:\n",
    "            print(\"Archivo correcto detectado para procesamiento.\")\n",
    "            process_and_upload_data(source_bucket_name, name, destination_bucket_name)\n",
    "        else:\n",
    "            print(f\"Archivo {name} no es relevante para este procesamiento. Procesamiento no necesario.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google maps🗺️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✅Etl de Google_review-florida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions_framework\n",
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "import os\n",
    "from io import BytesIO, StringIO\n",
    "import csv\n",
    "import re\n",
    "\n",
    "# Inicializa el cliente de Google Cloud Storage\n",
    "storage_client = storage.Client()\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Función para limpiar y formatear correctamente el texto, manejando comillas y caracteres especiales.\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return ''\n",
    "    text = text.replace('\"', '\"\"')  # Escapar comillas dobles\n",
    "    text = text.replace('\\n', ' ')  # Reemplazar saltos de línea por espacio\n",
    "    return text\n",
    "\n",
    "def process_and_upload_data(bucket_name, file_name, destination_bucket_name):\n",
    "    try:\n",
    "        print(f\"Procesando archivo desde {bucket_name}/{file_name}\")\n",
    "\n",
    "        # Descargar el archivo\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(file_name)\n",
    "        json_data = blob.download_as_bytes()\n",
    "        print(f\"Archivo {file_name} descargado del bucket {bucket_name}\")\n",
    "\n",
    "        # Verificar si los datos están vacíos\n",
    "        if not json_data:\n",
    "            raise ValueError(\"El archivo JSON está vacío.\")\n",
    "\n",
    "        # Cargar los datos en un DataFrame\n",
    "        temp_file = BytesIO(json_data)\n",
    "        df_reviews = pd.read_json(temp_file, lines=True, dtype={\"user_id\": str})\n",
    "\n",
    "        print(f\"Archivo {file_name} cargado en DataFrame\")\n",
    "\n",
    "        # Eliminar las columnas \"pics\" y \"resp\"\n",
    "        df_reviews = df_reviews.drop(columns=['pics', 'resp'], errors='ignore')\n",
    "\n",
    "        # Limpiar el texto para asegurar que las comillas estén correctamente manejadas\n",
    "        df_reviews['text'] = df_reviews['text'].apply(clean_text)\n",
    "\n",
    "        # Extraer el número del nombre del archivo original\n",
    "        match = re.search(r'(\\d+)', file_name)\n",
    "        if match:\n",
    "            number = match.group(1)\n",
    "            destination_blob_name = f'STAGED/Google/review-Florida/review-Florida{number}.csv'\n",
    "        else:\n",
    "            destination_blob_name = 'STAGED/Google/review-Florida.csv'\n",
    "\n",
    "        # Guardar los datos filtrados en un solo archivo CSV\n",
    "        buffer = StringIO()\n",
    "        df_reviews.to_csv(buffer, index=False, quoting=csv.QUOTE_ALL, escapechar='\\\\', doublequote=True)\n",
    "        buffer.seek(0)\n",
    "        destination_bucket = storage_client.bucket(destination_bucket_name)\n",
    "        destination_blob = destination_bucket.blob(destination_blob_name)\n",
    "        destination_blob.upload_from_string(buffer.getvalue(), content_type='text/csv')\n",
    "        print(f\"Datos cargados exitosamente a {destination_blob_name} en el bucket {destination_bucket_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "# Uso de variables de entorno para definir los parámetros\n",
    "source_bucket_name = os.getenv('SOURCE_BUCKET_NAME', 'henry-pizza')\n",
    "destination_bucket_name = os.getenv('DESTINATION_BUCKET_NAME', 'henry-pizza-staged')\n",
    "\n",
    "# Triggered by a change in a storage bucket\n",
    "@functions_framework.cloud_event\n",
    "def etl_google_review(cloud_event):\n",
    "    try:\n",
    "        data = cloud_event.data\n",
    "\n",
    "        event_id = cloud_event.get(\"id\")\n",
    "        event_type = cloud_event.get(\"type\")\n",
    "\n",
    "        bucket = data[\"bucket\"]\n",
    "        name = data[\"name\"]\n",
    "        metageneration = data[\"metageneration\"]\n",
    "        timeCreated = data[\"timeCreated\"]\n",
    "        updated = data[\"updated\"]\n",
    "\n",
    "        print(f\"Event ID: {event_id}\")\n",
    "        print(f\"Event type: {event_type}\")\n",
    "        print(f\"Bucket: {bucket}\")\n",
    "        print(f\"File: {name}\")\n",
    "        print(f\"Metageneration: {metageneration}\")\n",
    "        print(f\"Created: {timeCreated}\")\n",
    "        print(f\"Updated: {updated}\")\n",
    "\n",
    "        # Verificar si el archivo que disparó el evento es el archivo correcto\n",
    "        if re.search(r'RAW/Google/review[-_]Florida', name):\n",
    "            print(\"Archivo correcto detectado para procesamiento.\")\n",
    "            process_and_upload_data(source_bucket_name, name, destination_bucket_name)\n",
    "        else:\n",
    "            print(f\"Archivo {name} no es relevante para este procesamiento. Procesamiento no necesario.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✅Etl de Google_metadata-sitios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions_framework\n",
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "import os\n",
    "from io import BytesIO\n",
    "import re\n",
    "\n",
    "# Inicializa el cliente de Google Cloud Storage\n",
    "storage_client = storage.Client()\n",
    "\n",
    "def process_and_upload_data(bucket_name, file_name, destination_bucket_name):\n",
    "    try:\n",
    "        print(f\"Procesando archivo desde {bucket_name}/{file_name}\")\n",
    "\n",
    "        # Descargar el archivo\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(file_name)\n",
    "        json_data = blob.download_as_bytes()\n",
    "        print(f\"Archivo {file_name} descargado del bucket {bucket_name}\")\n",
    "\n",
    "        # Verificar si los datos están vacíos\n",
    "        if not json_data:\n",
    "            raise ValueError(\"El archivo JSON está vacío.\")\n",
    "\n",
    "        # Cargar los datos en un DataFrame\n",
    "        temp_file = BytesIO(json_data)\n",
    "        df_metadata = pd.read_json(temp_file, lines=True)\n",
    "        print(f\"Archivo {file_name} cargado en DataFrame\")\n",
    "        df_metadata = df_metadata.drop(columns=['price','hours','MISC','state','relative_results','url']) \n",
    "        # Filtros geográficos y categorías\n",
    "        latitud_max, latitud_min = 31.0, 24.5\n",
    "        longitud_max, longitud_min = -80.0, -87.6\n",
    "        mascara_latitud = (df_metadata['latitude'] >= latitud_min) & (df_metadata['latitude'] <= latitud_max)\n",
    "        mascara_longitud = (df_metadata['longitude'] >= longitud_min) & (df_metadata['longitude'] <= longitud_max)\n",
    "        df_filtered_FL = df_metadata[mascara_latitud & mascara_longitud].reset_index(drop=True)\n",
    "\n",
    "        # Reemplazar None por cadenas vacías en las columnas que se utilizarán para filtrar\n",
    "        df_filtered_FL['category'] = df_filtered_FL['category'].fillna('')\n",
    "        df_filtered_FL['name'] = df_filtered_FL['name'].fillna('')\n",
    "\n",
    "        # Filtrar pizzerías y Taco Bell\n",
    "        pizza_pattern = re.compile(r'pizza|pizzería', re.IGNORECASE)\n",
    "        taco_bell_pattern = re.compile(r'taco\\s*bell', re.IGNORECASE)\n",
    "\n",
    "        df_filtered = df_filtered_FL[\n",
    "            df_filtered_FL['category'].str.contains(pizza_pattern, na=False) | \n",
    "            df_filtered_FL['name'].apply(lambda x: bool(pizza_pattern.search(x))) |\n",
    "            df_filtered_FL['name'].apply(lambda x: bool(taco_bell_pattern.search(x)))\n",
    "        ]\n",
    "\n",
    "        # Convertir listas a cadenas de texto y manejar valores None\n",
    "        df_filtered.loc[:, 'category'] = df_filtered['category'].apply(lambda x: ', '.join(x) if isinstance(x, list) else (x if x is not None else ''))\n",
    "#         df_filtered.loc[:, 'hours'] = df_filtered['hours'].apply(lambda x: ', '.join([f\"{day}: {hours}\" for day, hours in x.items()]) if isinstance(x, dict) else (x if x is not None else '')) \n",
    "        # Guardar y cargar CSV con el mismo nombre en el bucket de destino\n",
    "        destination_blob_name = file_name.replace('RAW', 'STAGED').replace('.json', 'metadata-FL.csv')\n",
    "        buffer = BytesIO()\n",
    "        df_filtered.to_csv(buffer, index=False)\n",
    "        buffer.seek(0)\n",
    "        destination_bucket = storage_client.bucket(destination_bucket_name)\n",
    "        destination_blob = destination_bucket.blob(destination_blob_name)\n",
    "        destination_blob.upload_from_file(buffer, content_type='text/csv')\n",
    "        print(f\"Datos cargados exitosamente a {destination_blob_name} en el bucket {destination_bucket_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "# Uso de variables de entorno para definir los parámetros\n",
    "source_bucket_name = os.getenv('SOURCE_BUCKET_NAME', 'henry-pizza')\n",
    "destination_bucket_name = os.getenv('DESTINATION_BUCKET_NAME', 'henry-pizza-staged')\n",
    "\n",
    "# Triggered by a change in a storage bucket\n",
    "@functions_framework.cloud_event\n",
    "def etl_google_metadata(cloud_event):\n",
    "    try:\n",
    "        data = cloud_event.data\n",
    "\n",
    "        event_id = cloud_event.get(\"id\")\n",
    "        event_type = cloud_event.get(\"type\")\n",
    "\n",
    "        bucket = data.get(\"bucket\")\n",
    "        name = data.get(\"name\")\n",
    "        metageneration = data.get(\"metageneration\")\n",
    "        timeCreated = data.get(\"timeCreated\")\n",
    "        updated = data.get(\"updated\")\n",
    "\n",
    "        print(f\"Event ID: {event_id}\")\n",
    "        print(f\"Event type: {event_type}\")\n",
    "        print(f\"Bucket: {bucket}\")\n",
    "        print(f\"File: {name}\")\n",
    "        print(f\"Metageneration: {metageneration}\")\n",
    "        print(f\"Created: {timeCreated}\")\n",
    "        print(f\"Updated: {updated}\")\n",
    "\n",
    "        # Verificar si el archivo que disparó el evento es el archivo correcto\n",
    "        if name.startswith('RAW/Google/metadata-sitios'):\n",
    "            print(\"Archivo correcto detectado para procesamiento.\")\n",
    "            process_and_upload_data(source_bucket_name, name, destination_bucket_name)\n",
    "        else:\n",
    "            print(f\"Archivo {name} no es relevante para este procesamiento. Procesamiento no necesario.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📊Validación de datos y carga a BigQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- En esta etapa, nos encargamos de validar los datos mediante la creación de una `función` para normalizar los datos y una `tabla` en la que se hará un registro de todos los archivos que se subieron.\n",
    "- Tambièn se comentò cada etapa para poder visualizar el progreso en el servicio Loggins de Google Cloud Platform.\n",
    "- La verificación y validación de los datos evita el procesamiento innecesario de archivos no relevantes, esto, asegurando la ` la Integridad de los datos `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions_framework\n",
    "import pandas as pd\n",
    "from google.cloud import storage, bigquery\n",
    "import os\n",
    "from io import BytesIO, StringIO\n",
    "from datetime import datetime\n",
    "\n",
    "# Inicializa el cliente de Google Cloud Storage y BigQuery\n",
    "storage_client = storage.Client()\n",
    "bigquery_client = bigquery.Client()\n",
    "\n",
    "def validate_data(df, table_name):\n",
    "    \"\"\"\n",
    "    Función para validar los datos antes de cargarlos a BigQuery.\n",
    "    \"\"\"\n",
    "    if df.isnull().values.any():\n",
    "        print(f\"Warning: {table_name} contiene valores nulos.\")\n",
    "        df = df.dropna()\n",
    "    if df.duplicated().any():\n",
    "        print(f\"Warning: {table_name} contiene duplicados.\")\n",
    "        df = df.drop_duplicates()\n",
    "    return df\n",
    "\n",
    "def convert_column_types(df, table_name):\n",
    "    \"\"\"\n",
    "    Convierte las columnas a los tipos correctos según el esquema de BigQuery.\n",
    "    \"\"\"\n",
    "    if table_name == 'users':\n",
    "        df['user_id'] = df['user_id'].astype(str)\n",
    "        df['name'] = df['name'].astype(str)\n",
    "        df['review_count'] = df['review_count'].astype(int)\n",
    "        df['yelping_since'] = pd.to_datetime(df['yelping_since'], errors='coerce').dt.date\n",
    "        df['useful'] = df['useful'].astype(int)\n",
    "        df['funny'] = df['funny'].astype(int)\n",
    "        df['cool'] = df['cool'].astype(int)\n",
    "        df['fans'] = df['fans'].astype(int)\n",
    "        df['elite'] = df['elite'].astype(str)\n",
    "        df['average_stars'] = df['average_stars'].astype(float)\n",
    "        df['compliment_hot'] = df['compliment_hot'].astype(int)\n",
    "        df['compliment_more'] = df['compliment_more'].astype(int)\n",
    "        df['compliment_profile'] = df['compliment_profile'].astype(int)\n",
    "        df['compliment_cute'] = df['compliment_cute'].astype(int)\n",
    "        df['compliment_list'] = df['compliment_list'].astype(int)\n",
    "        df['compliment_note'] = df['compliment_note'].astype(int)\n",
    "        df['compliment_plain'] = df['compliment_plain'].astype(int)\n",
    "        df['compliment_cool'] = df['compliment_cool'].astype(int)\n",
    "        df['compliment_funny'] = df['compliment_funny'].astype(int)\n",
    "        df['compliment_writer'] = df['compliment_writer'].astype(int)\n",
    "        df['compliment_photos'] = df['compliment_photos'].astype(int)\n",
    "    elif table_name == 'businesses':\n",
    "        df['business_id'] = df['business_id'].astype(str)\n",
    "        df['name'] = df['name'].astype(str)\n",
    "        df['address'] = df['address'].astype(str)\n",
    "        df['city'] = df['city'].astype(str)\n",
    "        df['latitude'] = df['latitude'].astype(float)\n",
    "        df['longitude'] = df['longitude'].astype(float)\n",
    "        df['stars'] = df['stars'].astype(float)\n",
    "        df['review_count'] = df['review_count'].astype(int)\n",
    "        df['categories'] = df['categories'].astype(str)\n",
    "    elif table_name == 'reviews':\n",
    "        df['review_id'] = df['review_id'].astype(str)\n",
    "        df['user_id'] = df['user_id'].astype(str)\n",
    "        df['business_id'] = df['business_id'].astype(str)\n",
    "        df['stars'] = df['stars'].astype(float)\n",
    "        df['useful'] = df['useful'].astype(int)\n",
    "        df['funny'] = df['funny'].astype(int)\n",
    "        df['cool'] = df['cool'].astype(int)\n",
    "        df['text'] = df['text'].astype(str)\n",
    "        df['date'] = pd.to_datetime(df['date'], errors='coerce').dt.date\n",
    "    elif table_name == 'checkins':\n",
    "        df['business_id'] = df['business_id'].astype(str)\n",
    "        df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "    elif table_name == 'tips':\n",
    "        df['user_id'] = df['user_id'].astype(str)\n",
    "        df['business_id'] = df['business_id'].astype(str)\n",
    "        df['text'] = df['text'].astype(str)\n",
    "        df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "        df['compliment_count'] = df['compliment_count'].astype(int)\n",
    "    elif table_name == 'processed_files':\n",
    "        df['file_name'] = df['file_name'].astype(str)\n",
    "        df['load_timestamp'] = pd.to_datetime(df['load_timestamp'], errors='coerce')\n",
    "        df['file_size'] = df['file_size'].astype(int)\n",
    "        df['status'] = df['status'].astype(str)\n",
    "    elif table_name == 'metadata_sitios':\n",
    "        df['name'] = df['name'].astype(str)\n",
    "        df['address'] = df['address'].astype(str)\n",
    "        df['gmap_id'] = df['gmap_id'].astype(str)\n",
    "        df['description'] = df['description'].astype(str)\n",
    "        df['latitude'] = pd.to_numeric(df['latitude'])\n",
    "        df['longitude'] = pd.to_numeric(df['longitude'])\n",
    "        df['category'] = df['category'].astype(str)\n",
    "        df['avg_rating'] = pd.to_numeric(df['avg_rating'])\n",
    "        df['num_of_reviews'] = df['num_of_reviews'].astype(int)\n",
    "    elif table_name == 'review_florida':\n",
    "        df['user_id'] = df['user_id'].astype(str)    \n",
    "        df['name'] = df['name'].astype(str)    \n",
    "        df['time'] = pd.to_datetime(df['time'])    \n",
    "        df['rating'] = df['rating'].astype(float)    \n",
    "        df['text'] = df['text'].astype(str)\n",
    "        df['gmap_id'] = df['gmap_id'].astype(str)    \n",
    "\n",
    "    return df\n",
    "\n",
    "def filter_columns(df, table_name):\n",
    "    \"\"\"\n",
    "    Filtra las columnas del DataFrame según el esquema de BigQuery.\n",
    "    \"\"\"\n",
    "    if table_name == 'users':\n",
    "        expected_columns = ['user_id', 'name', 'review_count', 'yelping_since', 'useful', 'funny', 'cool', 'fans', 'elite', 'average_stars', 'compliment_hot', 'compliment_more', 'compliment_profile', 'compliment_cute', 'compliment_list', 'compliment_note', 'compliment_plain', 'compliment_cool', 'compliment_funny', 'compliment_writer', 'compliment_photos']\n",
    "    elif table_name == 'businesses':\n",
    "        expected_columns = ['business_id', 'name', 'address', 'city', 'latitude', 'longitude', 'stars', 'review_count', 'categories']\n",
    "    elif table_name == 'reviews':\n",
    "        expected_columns = ['review_id', 'user_id', 'business_id', 'stars','useful', 'funny', 'cool','text','date']\n",
    "    elif table_name == 'checkins':\n",
    "        expected_columns = ['business_id', 'date']\n",
    "    elif table_name == 'tips':\n",
    "        expected_columns = ['user_id', 'business_id', 'text', 'date','compliment_count']\n",
    "    elif table_name == 'processed_files':\n",
    "        expected_columns = ['file_name', 'load_timestamp', 'file_size', 'status']\n",
    "    elif table_name == 'metadata_sitios':\n",
    "        expected_columns = ['name','address','gmap_id','description','latitude','longitude','category','avg_rating','num_of_reviews']\n",
    "    elif table_name == 'review_florida':\n",
    "        expected_columns = ['user_id','name','time','rating','text','gmap_id']      \n",
    "    else:\n",
    "        expected_columns = df.columns.tolist()\n",
    "\n",
    "    return df[expected_columns]\n",
    "\n",
    "def load_data_to_bigquery(df, table_name):\n",
    "    \"\"\"\n",
    "    Función para cargar datos a BigQuery.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dataset_id = 'yelp_google_data'\n",
    "        table_id = f'{dataset_id}.{table_name}'\n",
    "\n",
    "        # Filtrar columnas que no están en el esquema de BigQuery\n",
    "        df = filter_columns(df, table_name)\n",
    "\n",
    "        job_config = bigquery.LoadJobConfig(\n",
    "            write_disposition=bigquery.WriteDisposition.WRITE_APPEND,\n",
    "            autodetect=True\n",
    "        )\n",
    "        \n",
    "        job = bigquery_client.load_table_from_dataframe(df, table_id, job_config=job_config)\n",
    "        job.result()\n",
    "        print(f\"Datos cargados exitosamente a {table_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading data to BigQuery: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def record_file_in_validation_table(file_name, file_size, status):\n",
    "    \"\"\"\n",
    "    Función para registrar el archivo en la tabla de validación.\n",
    "    \"\"\"\n",
    "    table_id = 'yelp_google_data.processed_files'\n",
    "    load_timestamp = datetime.utcnow().isoformat() + 'Z'\n",
    "    rows_to_insert = [\n",
    "        {\"file_name\": file_name, \"load_timestamp\": load_timestamp, \"file_size\": file_size, \"status\": status}\n",
    "    ]\n",
    "    errors = bigquery_client.insert_rows_json(table_id, rows_to_insert)\n",
    "    if errors:\n",
    "        raise RuntimeError(f\"Error inserting rows into validation table: {errors}\")\n",
    "    else:\n",
    "        print(f\"Archivo {file_name} registrado en la tabla de validación.\")\n",
    "\n",
    "def is_file_processed(file_name):\n",
    "    \"\"\"\n",
    "    Función para verificar si el archivo ya ha sido procesado.\n",
    "    \"\"\"\n",
    "    query = f\"\"\"\n",
    "        SELECT COUNT(*) as file_count\n",
    "        FROM `yelp_google_data.processed_files`\n",
    "        WHERE file_name = '{file_name}'\n",
    "    \"\"\"\n",
    "    result = bigquery_client.query(query).result().to_dataframe()\n",
    "    return result['file_count'][0] > 0\n",
    "\n",
    "def process_and_upload_data(bucket_name, source_blob_name, table_name):\n",
    "    try:\n",
    "        print(f\"Procesando archivo desde {bucket_name}/{source_blob_name}\")\n",
    "\n",
    "        # Verificar si el archivo ya ha sido procesado\n",
    "        if is_file_processed(source_blob_name):\n",
    "            print(f\"El archivo {source_blob_name} ya ha sido procesado. Procesamiento no necesario.\")\n",
    "            return\n",
    "\n",
    "        # Descargar el archivo\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(source_blob_name)\n",
    "        blob_data = blob.download_as_bytes()\n",
    "        file_size = blob.size\n",
    "\n",
    "        # Verificar si los datos están vacíos\n",
    "        if not blob_data:\n",
    "            raise ValueError(\"El archivo está vacío.\")\n",
    "\n",
    "        # Decodificar bytes a string\n",
    "        blob_data_str = blob_data.decode('utf-8')\n",
    "\n",
    "        # Procesar datos línea por línea para reducir el uso de memoria\n",
    "        data_stream = StringIO(blob_data_str)\n",
    "        df = pd.read_csv(data_stream)\n",
    "\n",
    "        # Validar datos\n",
    "        df = validate_data(df, table_name)\n",
    "\n",
    "        # Convertir los tipos de columna según el esquema de BigQuery\n",
    "        df = convert_column_types(df, table_name)\n",
    "\n",
    "        # Depurar el DataFrame antes de cargarlo a BigQuery\n",
    "        print(f\"Esquema del DataFrame antes de cargar a BigQuery:\\n{df.dtypes}\")\n",
    "        print(f\"Primeras filas del DataFrame:\\n{df.head()}\")\n",
    "\n",
    "        # Cargar datos a BigQuery\n",
    "        load_data_to_bigquery(df, table_name)\n",
    "\n",
    "        # Registrar archivo en tabla de validación\n",
    "        record_file_in_validation_table(source_blob_name, file_size, 'SUCCESS')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        # Registrar archivo en tabla de validación con estado de error\n",
    "        record_file_in_validation_table(source_blob_name, file_size, 'FAILED')\n",
    "        raise\n",
    "\n",
    "# Uso de variables de entorno para definir los parámetros\n",
    "source_bucket_name = os.getenv('SOURCE_BUCKET_NAME', 'henry-pizza')\n",
    "\n",
    "@functions_framework.cloud_event\n",
    "def big_query(cloud_event):\n",
    "    try:\n",
    "        data = cloud_event.data\n",
    "        bucket = data[\"bucket\"]\n",
    "        name = data[\"name\"]\n",
    "\n",
    "        print(f\"Bucket: {bucket}\")\n",
    "        print(f\"File: {name}\")\n",
    "\n",
    "        # Determinar la tabla de destino en función del archivo\n",
    "        if 'user' in name:\n",
    "            table_name = 'users'\n",
    "        elif 'business' in name:\n",
    "            table_name = 'businesses'\n",
    "        elif 'review_etl' in name:\n",
    "            table_name = 'reviews'\n",
    "        elif 'checkin' in name:\n",
    "            table_name = 'checkins'\n",
    "        elif 'tip' in name:\n",
    "            table_name = 'tips'\n",
    "        elif 'review-Florida' in name:\n",
    "            table_name = 'review_florida'  \n",
    "        elif 'metadata-FL' in name:\n",
    "            table_name = 'metadata_sitios'\n",
    "\n",
    "        else:\n",
    "            print(f\"Archivo {name} no es relevante para este procesamiento.\")\n",
    "            return\n",
    "\n",
    "        # Procesar y cargar datos\n",
    "        process_and_upload_data(bucket, name, table_name)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

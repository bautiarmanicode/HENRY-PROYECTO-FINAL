{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è **ETL (Extract, Transform, Load)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **üìÇProcesamiento del 1er archivo: `Google Maps/metadata-sitios/review-hawaii-`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  **Importamos las librer√≠as que vamos a usar**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "import data_utils   \n",
    "from data_utils import data_type_check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reviews Hawaii"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üì¶ **Extraccion** de los datos y primera exploraci√≥n \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üåü Primero, vamos a convertir los datos de las rese√±as de Google Maps del estado de Hawaii, distribuidos en 11 archivos JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>name</th>\n",
       "      <th>time</th>\n",
       "      <th>rating</th>\n",
       "      <th>text</th>\n",
       "      <th>pics</th>\n",
       "      <th>resp</th>\n",
       "      <th>gmap_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1281747</th>\n",
       "      <td>1.095291e+20</td>\n",
       "      <td>Christopher Salas</td>\n",
       "      <td>1522200055210</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0x7c006d9195964311:0x720889bdbbb0706d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500341</th>\n",
       "      <td>1.037525e+20</td>\n",
       "      <td>Paul Devich</td>\n",
       "      <td>1604988848336</td>\n",
       "      <td>5</td>\n",
       "      <td>Got a ultimate car wash</td>\n",
       "      <td>None</td>\n",
       "      <td>{'time': 1620353852050, 'text': 'MAHALO for us...</td>\n",
       "      <td>0x79540e567a7fe245:0x8892f29c46794f43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              user_id               name           time  rating  \\\n",
       "1281747  1.095291e+20  Christopher Salas  1522200055210       4   \n",
       "500341   1.037525e+20        Paul Devich  1604988848336       5   \n",
       "\n",
       "                            text  pics  \\\n",
       "1281747                     None  None   \n",
       "500341   Got a ultimate car wash  None   \n",
       "\n",
       "                                                      resp  \\\n",
       "1281747                                               None   \n",
       "500341   {'time': 1620353852050, 'text': 'MAHALO for us...   \n",
       "\n",
       "                                       gmap_id  \n",
       "1281747  0x7c006d9195964311:0x720889bdbbb0706d  \n",
       "500341   0x79540e567a7fe245:0x8892f29c46794f43  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se especifica la ruta que contiene la carpeta con los archivos:\n",
    "carpeta = \"../0_Dataset/review-Hawaii\"\n",
    "\n",
    "# Se crea lista vac√≠a donde se almacenar√°n los dataframes de cada archivo:\n",
    "reviews = []\n",
    "\n",
    "# Se recorre por todos los archivos en la carpeta:\n",
    "for filename in os.listdir(carpeta):\n",
    "    if filename.endswith('.json'):\n",
    "        # Se carga el archivo JSON en un DataFrame de Pandas:\n",
    "        filepath = os.path.join(carpeta, filename)\n",
    "        df = pd.read_json(filepath, lines = True)\n",
    "        \n",
    "        # Se agrega el DataFrame a la lista:\n",
    "        reviews.append(df)\n",
    "\n",
    "# Se combinan todos los DataFrames en uno solo usando pd.concat:\n",
    "df_rev_hawai = pd.concat(reviews, ignore_index=True)\n",
    "\n",
    "# Se chequean 2 valores:\n",
    "df_rev_hawai.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Usando la funcion personalizada `data_type_check` invocada desde `data_utils.py` podemos observar:\n",
    "- Variables categ√≥ricas\n",
    "- Variables num√©ricas\n",
    "- Dimensiones del dataframe\n",
    "- Nulos\n",
    "- Tipos de datos\n",
    "- Informacion acerca de los datos faltantes o nulos de cada columna    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      " Resumen del dataframe:\n",
      "\n",
      "========================================\n",
      "Dimensiones:  (1504347, 8)\n",
      "   columna  %_no_nulos  %_nulos  total_nulos tipo_dato\n",
      "0  user_id      100.00     0.00            0   float64\n",
      "1     name      100.00     0.00            0    object\n",
      "2     time      100.00     0.00            0     int64\n",
      "3   rating      100.00     0.00            0     int64\n",
      "4     text       56.68    43.32       651751    object\n",
      "5     pics        8.82    91.18      1371645    object\n",
      "6     resp        7.23    92.77      1395548    object\n",
      "7  gmap_id      100.00     0.00            0    object\n"
     ]
    }
   ],
   "source": [
    "data_type_check(df_rev_hawai)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **üì§ LOAD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60174, 8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rev_hawai_muestra = df_rev_hawai.sample(frac=0.50, random_state=1) \n",
    "df_rev_hawai_muestra.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divido el parquet en 2 muestras \n",
    "#df_rev_hawai_muestra = df_rev_hawai[:int(len(df_rev_hawai)/2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se exporta el archivo en formato parquet:\n",
    "df_rev_hawai_muestra.to_parquet(\"../0_Dataset/Google_rev_hawaii_muestra.parquet\", engine=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quizas borrar\n",
    "\n",
    "# Se buscan los valores √∫nicos en gmap_id:\n",
    "# df_rev_hawai_muestra = df_rev_hawai_muestra.dropna(subset=\"gmap_id\")\n",
    "# id_unicos = df_rev_hawai_muestra[\"gmap_id\"].unique()\n",
    "# id_unicos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadata-sitios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üåü La \"metadata\" incluye la informaci√≥n de diversos establecimientos en Google Maps.\n",
    "\n",
    "üìÇ Esta informaci√≥n est√° dividida en 11 archivos JSON, organizados en 3 carpetas para facilitar el procesamiento y almacenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mEl kernel se bloque√≥ al ejecutar c√≥digo en la celda actual o en una celda anterior. \n",
      "\u001b[1;31mRevise el c√≥digo de las celdas para identificar una posible causa del error. \n",
      "\u001b[1;31mHaga clic <a href='https://aka.ms/vscodeJupyterKernelCrash'>aqu√≠</a> para obtener m√°s informaci√≥n. \n",
      "\u001b[1;31mVea Jupyter <a href='command:jupyter.viewOutput'>log</a> para obtener m√°s detalles."
     ]
    }
   ],
   "source": [
    "# Se especifica la ruta que contiene la carpeta con los archivos:\n",
    "carpeta = \"../0_Dataset/metadata-sitios\"\n",
    "\n",
    "# Se crea lista vac√≠a donde se almacenar√°n los dataframes de cada archivo:\n",
    "metadata = []\n",
    "\n",
    "# Se recorre por todos los archivos en la carpeta:\n",
    "for filename in os.listdir(carpeta):\n",
    "    if filename.endswith('.json'):\n",
    "        # Se carga el archivo JSON en un DataFrame de Pandas:\n",
    "        filepath = os.path.join(carpeta, filename)\n",
    "        df1 = pd.read_json(filepath, lines = True)\n",
    "        \n",
    "        # Se agrega el DataFrame a la lista:\n",
    "        metadata.append(df1)\n",
    "\n",
    "# Se combinan todos los DataFrames en uno solo usando pd.concat:\n",
    "df_md_hawaii = pd.concat(metadata, ignore_index=True)\n",
    "\n",
    "# Se chequean 2 valores:\n",
    "df_md_hawaii.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Usando la funcion personalizada `data_type_check` invocada desde `data_utils.py` podemos observar:\n",
    "- Variables categ√≥ricas\n",
    "- Variables num√©ricas\n",
    "- Dimensiones del dataframe\n",
    "- Nulos\n",
    "- Tipos de datos\n",
    "- Informacion acerca de los datos faltantes o nulos de cada columna    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      " Resumen del dataframe:\n",
      "\n",
      "========================================\n",
      "Dimensiones:  (2475009, 15)\n",
      "             columna  %_no_nulos  %_nulos  total_nulos tipo_dato\n",
      "0               name      100.00     0.00           33    object\n",
      "1            address       97.19     2.81        69648    object\n",
      "2            gmap_id      100.00     0.00            0    object\n",
      "3        description        7.53    92.47      2288605    object\n",
      "4           latitude      100.00     0.00            0   float64\n",
      "5          longitude      100.00     0.00            0   float64\n",
      "6           category       99.39     0.61        15163    object\n",
      "7         avg_rating      100.00     0.00            0   float64\n",
      "8     num_of_reviews      100.00     0.00            0     int64\n",
      "9              price        8.12    91.88      2273984    object\n",
      "10             hours       73.19    26.81       663517    object\n",
      "11              MISC       76.15    23.85       590243    object\n",
      "12             state       74.56    25.44       629696    object\n",
      "13  relative_results       89.88    10.12       250397    object\n",
      "14               url      100.00     0.00            0    object\n"
     ]
    }
   ],
   "source": [
    "data_type_check(df_md_hawaii)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **üîÑ TRANSFORM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Limpieza pendiente, es posible que lo hagamos en la nube y ya.\n",
    "- Falta duplicados\n",
    "- tipos de datos\n",
    "- valores faltantes\n",
    "- nulos\n",
    "- duplicados\n",
    "- metadatos/dicc datos\n",
    "- outliers\n",
    "- distribuci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **üì§ LOAD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_md_hawaii_muestra = df_md_hawaii.sample(frac=0.30, random_state=1) \n",
    "df_md_hawaii_muestra.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divido el parquet en 2 muestras para poder subirlo a github y mostralo al equipo\n",
    "# df_md_hawaii_muestra = df_md_hawaii[:int(len(df_md_hawaii)/10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se exporta el archivo en formato parquet:\n",
    "df_md_hawaii_muestra.to_parquet(\"../0_Dataset/Google_metadata_california_muestra.parquet\", engine=\"pyarrow\")\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Posibles soluciones: Exportar multiples parquet de metadata y unirlos luego \n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
